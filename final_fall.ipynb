{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b632b5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.5.0 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (2.5.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (4.6.0.66)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\rijo\\anaconda3\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: twilio in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (7.16.3)\n",
      "Requirement already satisfied: moviepy in c:\\users\\rijo\\anaconda3\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: librosa in c:\\users\\rijo\\anaconda3\\lib\\site-packages (0.9.2)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.5.0) (2.11.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.5.0) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.5.0) (3.20.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.5.0) (3.3.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.5.0) (1.1.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.5.0) (1.15.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (1.12.1)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.5.0) (0.15.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (0.37.1)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.5.0) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.5.0) (3.7.4.3)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.5.0) (1.6.3)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.5.0) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.5.0) (1.1.2)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.5.0) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.5.0) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.5.0) (2.5.0)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.5.0) (0.4.0)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.5.0) (1.34.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: pytz in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from twilio) (2022.1)\n",
      "Requirement already satisfied: PyJWT<3.0.0,>=2.0.0 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from twilio) (2.4.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from twilio) (2.28.1)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from moviepy) (0.4.8)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from moviepy) (2.19.3)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from moviepy) (4.64.1)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from librosa) (1.6.0)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from librosa) (0.11.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from librosa) (3.0.0)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from librosa) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from librosa) (0.24.2)\n",
      "Requirement already satisfied: resampy>=0.2.2 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from librosa) (0.4.2)\n",
      "Requirement already satisfied: numba>=0.45.1 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from librosa) (0.55.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from numba>=0.45.1->librosa) (63.4.1)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from numba>=0.45.1->librosa) (0.38.0)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from requests>=2.0.0->twilio) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from requests>=2.0.0->twilio) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from requests>=2.0.0->twilio) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from requests>=2.0.0->twilio) (3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->librosa) (2.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.15.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.0.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.6.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\rijo\\appdata\\roaming\\python\\python39\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.5.0 opencv-python matplotlib   twilio moviepy librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce664908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sounddevice\n",
      "  Downloading sounddevice-0.4.6-py3-none-win_amd64.whl (199 kB)\n",
      "     -------------------------------------- 199.7/199.7 kB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from sounddevice) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rijo\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
      "Installing collected packages: sounddevice\n",
      "Successfully installed sounddevice-0.4.6\n"
     ]
    }
   ],
   "source": [
    "!pip install sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b3fa43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import sounddevice as sd\n",
    "import featurecalc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed59400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter=tf.lite.Interpreter(model_path='lite-model_movenet_singlepose_lightning_3.tflite')\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca897498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate centroid from keypoints\n",
    "def calculate_centroid(keypoints):\n",
    "    x_coords = keypoints[:, 0]\n",
    "    y_coords = keypoints[:, 1]\n",
    "    centroid_x = np.mean(x_coords)\n",
    "    centroid_y = np.mean(y_coords)\n",
    "    return np.array([centroid_x, centroid_y])\n",
    "\n",
    "# Function to calculate angles between keypoints\n",
    "def calculate_angles(keypoints):\n",
    "    # Calculate angles between hip, knee, and ankle keypoints for each leg\n",
    "    left_hip, left_knee, left_ankle = keypoints[11], keypoints[12], keypoints[13]\n",
    "    right_hip, right_knee, right_ankle = keypoints[8], keypoints[9], keypoints[10]\n",
    "    left_leg_angle = np.arctan2(left_ankle[1] - left_knee[1], left_ankle[0] - left_knee[0]) \\\n",
    "                      - np.arctan2(left_hip[1] - left_knee[1], left_hip[0] - left_knee[0])\n",
    "    right_leg_angle = np.arctan2(right_ankle[1] - right_knee[1], right_ankle[0] - right_knee[0]) \\\n",
    "                       - np.arctan2(right_hip[1] - right_knee[1], right_hip[0] - right_knee[0])\n",
    "    # Calculate angles between shoulder, elbow, and wrist keypoints for each arm\n",
    "    left_shoulder, left_elbow, left_wrist = keypoints[5], keypoints[6], keypoints[7]\n",
    "    right_shoulder, right_elbow, right_wrist = keypoints[2], keypoints[3], keypoints[4]\n",
    "    left_arm_angle = np.arctan2(left_wrist[1] - left_elbow[1], left_wrist[0] - left_elbow[0]) \\\n",
    "                      - np.arctan2(left_elbow[1] - left_shoulder[1], left_elbow[0] - left_shoulder[0])\n",
    "    right_arm_angle = np.arctan2(right_wrist[1] - right_elbow[1], right_wrist[0] - right_elbow[0]) \\\n",
    "                       - np.arctan2(right_elbow[1] - right_shoulder[1], right_elbow[0] - right_shoulder[0])\n",
    "    return np.array([left_leg_angle, right_leg_angle, left_arm_angle, right_arm_angle])\n",
    "\n",
    "# Function to check if feet keypoints are in contact with the ground\n",
    "def check_feet_contact(keypoints):\n",
    "    left_foot, right_foot = keypoints[14], keypoints[11]\n",
    "    if left_foot[1] > right_foot[1]:\n",
    "        lower_foot = right_foot\n",
    "        higher_foot = left_foot\n",
    "    else:\n",
    "        lower_foot = left_foot\n",
    "        higher_foot = right_foot\n",
    "    return lower_foot[1] <= higher_foot[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d8b323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drawing the keypoints\n",
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for kp in shaped:\n",
    "        ky, kx, kp_conf = kp\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 4, (0,255,0), -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0fd9e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This list represents all the different connections present in our body\n",
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9201a92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "        \n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "286586ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7180\\1944639551.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeypoints\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m51\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mkeypoints_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeypoints_with_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m     \u001b[0mcentroid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeaturecalc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_centroid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeypoints_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m     \u001b[0mkeypoints_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeypoints_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mkeypoints_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeypoints_data\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# remove dimensions of size 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\featurecalc.py\u001b[0m in \u001b[0;36mcalculate_centroid\u001b[1;34m(keypoints)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_centroid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeypoints\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mx_coords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeypoints\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0my_coords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeypoints\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mcentroid_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_coords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mcentroid_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_coords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Set up real-time capturing of audio\n",
    "# Create a PyAudio object for audio capturing\n",
    "rate = 44100  # Sample rate 16KHz for usb microphone\n",
    "channels = 1  # Number of channels (mono) 1\n",
    "format = pyaudio.paFloat32  # Format of the audio data pyaudio.paInt16\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "stream = p.open(format=format,\n",
    "                channels=channels,\n",
    "                rate=rate,\n",
    "                input=True,\n",
    "                frames_per_buffer=1024)\n",
    "\n",
    "# Set up real-time capturing of video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    " \n",
    "file_path = 'E:/sightica/final_dataset/clf.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "        clf = pickle.load(f)\n",
    "        \n",
    "    \n",
    "    \n",
    "file_path = 'E:/sightica/final_dataset/gradient_booster.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "        gradient_booster = pickle.load(f)\n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "# Loop for real-time processing\n",
    "while True:\n",
    "    # Capture video frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Extract video features from frame\n",
    "   # Reshape image\n",
    "    img = frame.copy()\n",
    "    \n",
    "     #Video to audio\n",
    "   \n",
    "    \n",
    "    img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 192,192)\n",
    "    input_image = tf.cast(img, dtype=tf.float32)\n",
    "    \n",
    "    # Setup input and output \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Make predictions \n",
    "    interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "    interpreter.invoke()\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    keypoints=keypoints_with_scores.reshape([17,3])\n",
    "    key=keypoints.reshape([1,51])\n",
    "    keypoints_data = keypoints_with_scores.reshape([17,3])[:, :2]\n",
    "    centroid = featurecalc.calculate_centroid(keypoints_data)\n",
    "    keypoints_data = np.array(keypoints_data)\n",
    "    keypoints_data = np.squeeze(keypoints_data)  # remove dimensions of size 1\n",
    "    angles = featurecalc.calculate_angles(keypoints_data)\n",
    "    feet_contact = featurecalc.check_feet_contact(keypoints_data)\n",
    "    features = np.concatenate((centroid, angles, [feet_contact]))\n",
    "    feature_vector = np.concatenate((keypoints_data.flatten(), features), axis=0)\n",
    "    feature=feature_vector.reshape(1,-1)\n",
    "    x_test1=feature\n",
    "    \n",
    "    # Load audio file\n",
    "    data = stream.read(1024)\n",
    "    data_np = np.frombuffer(data, dtype=np.float32)\n",
    "    audio_data = np.concatenate((audio_data, data_np), axis=0)\n",
    "   \n",
    "\n",
    "    # Extract audio features from audio_data\n",
    "    audio_features = np.mean(librosa.feature.mfcc(y=audio_data, sr=rate,n_mfcc=50).T, axis=0)  # Extract features from audio_data\n",
    "    result=audio_features.reshape([1,50])\n",
    "    x_test2=result\n",
    "\n",
    "    # Make predictions with audio and video models\n",
    "    #audio_pred =gradient_booster.predict(x_test2)\n",
    "    video_pred =clf.predict(x_test1)\n",
    "    audio_pred=gradient_booster.predict(x_test2)\n",
    "    # Implement decision logic\n",
    "    if audio_pred and  video_pred == 1:\n",
    "        print(\"Fall detected!\")\n",
    "\n",
    "    # Perform real-time actions based on the fall detection prediction\n",
    "    # (e.g., trigger alarm, send notifications, etc.)\n",
    "    \n",
    "    # Display video frame\n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to stop the loop\n",
    "        break\n",
    "\n",
    "# Clean up resources\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "802cd20c",
   "metadata": {},
   "outputs": [
    {
     "ename": "PortAudioError",
     "evalue": "Can't read from a callback stream [PaErrorCode -9977]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPortAudioError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17028\\2288764155.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m      \u001b[1;31m# Capture audio data from the audio stream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0maudio_data_bytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maudio_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mduration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Convert bytes to NumPy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sounddevice.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, frames)\u001b[0m\n\u001b[0;32m   1454\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1455\u001b[0m         \u001b[0mchannels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_channels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1456\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverflowed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRawInputStream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1457\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1458\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverflowed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sounddevice.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, frames)\u001b[0m\n\u001b[0;32m   1234\u001b[0m             \u001b[0moverflowed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m             \u001b[0m_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m             \u001b[0moverflowed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverflowed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sounddevice.py\u001b[0m in \u001b[0;36m_check\u001b[1;34m(err, msg)\u001b[0m\n\u001b[0;32m   2745\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mPortAudioError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrormsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhosterror_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2747\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mPortAudioError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrormsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPortAudioError\u001b[0m: Can't read from a callback stream [PaErrorCode -9977]"
     ]
    }
   ],
   "source": [
    "display_flag=False\n",
    "os.chdir('E:/sightica/')\n",
    "frame_buffer = []\n",
    "counter = 0\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "     # Capture audio data from the audio stream\n",
    "    audio_data_bytes = audio_stream.read(int(sr * duration))\n",
    "\n",
    "# Convert bytes to NumPy array\n",
    "    audio_data = np.frombuffer(audio_data_bytes, dtype=np.int16)\n",
    "\n",
    "    \n",
    "    # Reshape image\n",
    "    img = frame.copy()\n",
    "    \n",
    "     #Video to audio\n",
    "   \n",
    "    \n",
    "    img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 192,192)\n",
    "    input_image = tf.cast(img, dtype=tf.float32)\n",
    "    \n",
    "    # Setup input and output \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Make predictions \n",
    "    interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "    interpreter.invoke()\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    keypoints=keypoints_with_scores.reshape([17,3])\n",
    "    key=keypoints.reshape([1,51])\n",
    "    keypoints_data = keypoints_with_scores.reshape([17,3])[:, :2]\n",
    "    centroid = calculate_centroid(keypoints_data)\n",
    "    keypoints_data = np.array(keypoints_data)\n",
    "    keypoints_data = np.squeeze(keypoints_data)  # remove dimensions of size 1\n",
    "    angles = calculate_angles(keypoints_data)\n",
    "    feet_contact = check_feet_contact(keypoints_data)\n",
    "    features = np.concatenate((centroid, angles, [feet_contact]))\n",
    "    feature_vector = np.concatenate((keypoints_data.flatten(), features), axis=0)\n",
    "    feature=feature_vector.reshape(1,-1)\n",
    "    x_test1=feature\n",
    "    file_path = 'E:/sightica/final_dataset/clf.pkl'\n",
    "    with open(file_path, 'rb') as f:\n",
    "        clf = pickle.load(f)\n",
    "        y_pred1 = clf.predict(x_test1)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(audio_data, sr=sr,n_mfcc=50).T, axis=0)\n",
    "    result=mfccs.reshape([1,50])\n",
    "    x_test2=result\n",
    "    file_path = 'E:/sightica/final_dataset/gradient_booster.pkl'\n",
    "    with open(file_path, 'rb') as f:\n",
    "        gradient_booster = pickle.load(f)\n",
    "        y_pred2 = gradient_booster.predict(x_test2)\n",
    "   \n",
    "    \n",
    "   \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "   # if y_pred1==1 and display_flag==True:\n",
    "    if y_pred1==1:\n",
    "    \n",
    "            draw_connections(frame, keypoints_with_scores, EDGES, 0.4)\n",
    "            draw_keypoints(frame, keypoints_with_scores, 0.4)\n",
    "            cv2.putText(frame, \n",
    "                ' Fall', \n",
    "                (50, 50), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, \n",
    "                (0, 255, 255), \n",
    "                2, \n",
    "                cv2.LINE_4)\n",
    "            cv2.imshow(\"Fall Detected\", frame)\n",
    "            \n",
    "            # Capture the frame and note the time\n",
    "            if not os.path.exists('fall_images'):\n",
    "                 os.mkdir('fall_images')\n",
    "            img_name = f'fall_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.jpg'\n",
    "            img_path = os.path.join('fall_images', img_name)\n",
    "            cv2.imwrite(img_path, frame)\n",
    "            with open('fall_times.txt', 'a') as f:\n",
    "                f.write(f'Fall detected at {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "   \n",
    "   \n",
    "    if y_pred1==0:\n",
    "    \n",
    "           \n",
    "            draw_connections(frame, keypoints_with_scores, EDGES, 0.4)\n",
    "            draw_keypoints(frame, keypoints_with_scores, 0.4)\n",
    "            cv2.putText(frame, \n",
    "                ' Not Fall', \n",
    "                (50, 50), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, \n",
    "                (0, 255, 255), \n",
    "                2, \n",
    "                cv2.LINE_4)\n",
    "            cv2.imshow(\"Fall Detected\", frame)\n",
    "       \n",
    "   \n",
    "    #Check for key press to exit the loop\n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "    #key = cv2.waitKey(1)\n",
    "    #if key == ord(\"q\"):\n",
    "        #break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "audio_stream.stop()\n",
    "audio_stream.close()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "956a1945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected for 5 subsequent frames\n"
     ]
    }
   ],
   "source": [
    "display_flag=False\n",
    "os.chdir('E:/sightica/')\n",
    "frame_buffer = []\n",
    "counter = 0\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Reshape image\n",
    "    img = frame.copy()\n",
    "    \n",
    "     #Video to audio\n",
    "   \n",
    "    \n",
    "    img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 192,192)\n",
    "    input_image = tf.cast(img, dtype=tf.float32)\n",
    "    \n",
    "    # Setup input and output \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Make predictions \n",
    "    interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "    interpreter.invoke()\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    keypoints=keypoints_with_scores.reshape([17,3])\n",
    "    key=keypoints.reshape([1,51])\n",
    "    keypoints_data = keypoints_with_scores.reshape([17,3])[:, :2]\n",
    "    centroid = calculate_centroid(keypoints_data)\n",
    "    keypoints_data = np.array(keypoints_data)\n",
    "    keypoints_data = np.squeeze(keypoints_data)  # remove dimensions of size 1\n",
    "    angles = calculate_angles(keypoints_data)\n",
    "    feet_contact = check_feet_contact(keypoints_data)\n",
    "    features = np.concatenate((centroid, angles, [feet_contact]))\n",
    "    feature_vector = np.concatenate((keypoints_data.flatten(), features), axis=0)\n",
    "    feature=feature_vector.reshape(1,-1)\n",
    "    x_test1=feature\n",
    "    file_path = 'E:/sightica/final_dataset/clf.pkl'\n",
    "    with open(file_path, 'rb') as f:\n",
    "        clf = pickle.load(f)\n",
    "        y_pred1 = clf.predict(x_test1)\n",
    "    \n",
    "    \n",
    "    frame_buffer.append(y_pred1)\n",
    "    \n",
    "    if len(frame_buffer) > 5:\n",
    "        # Pop the oldest frame from the buffer\n",
    "        frame_buffer.pop(0)\n",
    "\n",
    "    # Check if all frames in the buffer indicate a fall (i.e., y_pred1 is 1)\n",
    "    if all(frame == 1 for frame in frame_buffer):\n",
    "        counter += 1\n",
    "    else:\n",
    "        counter = 0\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "   # if y_pred1==1 and display_flag==True:\n",
    "    #if y_pred1==1:\n",
    "    if counter == 5:\n",
    "            display_flag=True\n",
    "            print(\"Fall detected for 5 subsequent frames\")\n",
    "            draw_connections(frame, keypoints_with_scores, EDGES, 0.4)\n",
    "            draw_keypoints(frame, keypoints_with_scores, 0.4)\n",
    "            cv2.putText(frame, \n",
    "                ' Fall', \n",
    "                (50, 50), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, \n",
    "                (0, 255, 255), \n",
    "                2, \n",
    "                cv2.LINE_4)\n",
    "            cv2.imshow(\"Fall Detected\", frame)\n",
    "            \n",
    "            # Capture the frame and note the time\n",
    "            if not os.path.exists('fall_images'):\n",
    "                 os.mkdir('fall_images')\n",
    "            img_name = f'fall_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.jpg'\n",
    "            img_path = os.path.join('fall_images', img_name)\n",
    "            cv2.imwrite(img_path, frame)\n",
    "            with open('fall_times.txt', 'a') as f:\n",
    "                f.write(f'Fall detected at {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "    elif display_flag:\n",
    "        cv2.destroyAllWindows()\n",
    "        display_flag = False\n",
    " \n",
    "            \n",
    "   # if y_pred1==0 and display_flag:\n",
    "    #if y_pred1==0:\n",
    "    #else:\n",
    "            #display_flag=False\n",
    "           # print(\"No fall\")\n",
    "            #draw_connections(frame, keypoints_with_scores, EDGES, 0.4)\n",
    "            #draw_keypoints(frame, keypoints_with_scores, 0.4)\n",
    "            #cv2.putText(frame, \n",
    "             #   ' Not Fall', \n",
    "              #  (50, 50), \n",
    "               # cv2.FONT_HERSHEY_SIMPLEX, 1, \n",
    "                #(0, 255, 255), \n",
    "                #2, \n",
    "                #cv2.LINE_4)\n",
    "            #cv2.imshow(\"Fall Detected\", frame)\n",
    "        #cv2.destroyWindow(\"Fall Detected\")\n",
    "    # Display frames and update key press event\n",
    "    cv2.imshow(\"Camera\", frame)\n",
    "    #Check for key press to exit the loop\n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "    #key = cv2.waitKey(1)\n",
    "    #if key == ord(\"q\"):\n",
    "        #break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
